{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/frood/sources/python/deeplearn/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import ipdb\n",
    "from typing import List, Dict, Union\n",
    "import wandb\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import transformers\n",
    "from datasets import load_dataset, load_from_disk\n",
    "\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "torch.set_printoptions(threshold=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing on cuda\n"
     ]
    }
   ],
   "source": [
    "class Parameters():\n",
    "    def __init__(self):\n",
    "        # training parameters\n",
    "        self.batch_size = 1\n",
    "        self.learning_rate = 6e-5\n",
    "        self.epochs = 3\n",
    "        self.lr_warmup_steps = 100\n",
    "        self.context_length = 1024\n",
    "        self.alpha = 0.5 # weighting for PRPO odds ratio\n",
    "        self.prompt_max_length = 512\n",
    "        self.compile = False\n",
    "        self.dtype = torch.float16\n",
    "        self.log_iters = 50\n",
    "\n",
    "        # hyperparameters\n",
    "        self.dropout = 0.0\n",
    "        self.grad_clip = 1.0\n",
    "        self.weight_decay = 0.0\n",
    "\n",
    "        # device setup\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        # logging\n",
    "        self.wandb = True\n",
    "        self.wanadb_project_name = \"aligntest\"\n",
    "        self.wandb_project = self.wanadb_project_name\n",
    "        self.wandb_run_name = f\"{self.wanadb_project_name}-run-{datetime.now().strftime('%Y_%m_%d_%H_%M_%S')}\"\n",
    "        self.wandb_api_key = \"***\" # paste a valid API key\n",
    "\n",
    "    def wanadb_init(self):\n",
    "        wandb.login(key=self.wandb_api_key)\n",
    "        wandb.init(project=self.wandb_project, name=self.wandb_run_name)\n",
    "        \n",
    "\n",
    "parameters = Parameters()\n",
    "parameters.wanadb_init()\n",
    "\n",
    "print(f\"Computing on {parameters.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"files/data/orpo_dataset\"\n",
    "dataset_name = \"mlabonne/orpo-dpo-mix-40k\"\n",
    "tokenizer_path = \"files/tokenizers/tok16384\"\n",
    "checkpoint_dir = \"files/models/\"\n",
    "\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(tokenizer_path)\n",
    "\n",
    "with open('chat.dtpl', 'r', encoding='utf-8') as file:\n",
    "    tokenizer.chat_template = file.read()\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "if os.path.exists(dataset_path):\n",
    "    dataset = load_from_disk(dataset_path)\n",
    "else:\n",
    "    print(\"Filtering and tokenizing dataset\")\n",
    "    dataset = load_dataset(dataset_name, split=\"all\")\n",
    "    \n",
    "    # optionally filter out some of the entries (37136 vs 36622)\n",
    "    dataset.filter(lambda x: x[\"source\"] != \"toxic-dpo-v0.2\")\n",
    "\n",
    "    # Filter dataset\n",
    "    # Eliminate entries longer than 512 (prompt_max_length). This is important\n",
    "    # because we want the prompt + answer to fit within the context_length\n",
    "    def filter_dataset(examples: Dict[str, Union[str, List[str]]]) -> bool:\n",
    "        prompt = tokenizer.apply_chat_template(\n",
    "            examples[\"chosen\"][:-1],\n",
    "            tokenize=True,\n",
    "            add_generation_prompt=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        prompt_length = prompt.size(-1)\n",
    "        if prompt_length < parameters.prompt_max_length:\n",
    "            return True\n",
    "        \n",
    "        return False\n",
    "    \n",
    "    # Preprocess and tokenize dataset\n",
    "    def preprocess_dataset(examples: Dict[str, Union[str, List[str]]]) -> Dict[\n",
    "        str, Union[str, List[str]]\n",
    "    ]:\n",
    "        prompt = [tokenizer.apply_chat_template(\n",
    "            item[:-1], tokenize=False, \n",
    "            add_generation_prompt=True\n",
    "        ) for item in examples[\"chosen\"]]\n",
    "\n",
    "        chosen = [tokenizer.apply_chat_template(\n",
    "            item, tokenize=False\n",
    "        ) for item in examples[\"chosen\"]]\n",
    "\n",
    "        rejected = [tokenizer.apply_chat_template(\n",
    "            item, tokenize=False\n",
    "        ) for item in examples[\"rejected\"]]\n",
    "\n",
    "        inputs = tokenizer(\n",
    "            prompt, max_length=parameters.context_length, \n",
    "            padding=\"max_length\", truncation=True, return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        pos_labels = tokenizer(\n",
    "            chosen, max_length=parameters.context_length, \n",
    "            padding=\"max_length\", truncation=True, return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        neg_labels = tokenizer(\n",
    "            rejected, max_length=parameters.context_length, \n",
    "            padding=\"max_length\", truncation=True, return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        inputs[\"positive_input_ids\"] = pos_labels[\"input_ids\"]\n",
    "        inputs[\"positive_attention_mask\"] = pos_labels[\"attention_mask\"]\n",
    "\n",
    "        inputs[\"negative_input_ids\"] = neg_labels[\"input_ids\"]\n",
    "        inputs[\"negative_attention_mask\"] = neg_labels[\"attention_mask\"]\n",
    "\n",
    "        return inputs\n",
    "\n",
    "    # exclude propts that are too long\n",
    "    dataset = dataset.filter(filter_dataset)\n",
    "    \n",
    "    dataset = dataset.map(\n",
    "        preprocess_dataset, batched=True, \n",
    "        num_proc=min(32, os.cpu_count()),\n",
    "        remove_columns=dataset.column_names\n",
    "    )\n",
    "\n",
    "    dataset.save_to_disk(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(dataset[2][\"positive_input_ids\"]))\n",
    "# print(dataset[2][\"positive_input_ids\"])\n",
    "# tokenizer.decode(dataset[2][\"positive_input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_split = dataset.shuffle(42).train_test_split(test_size=0.05)\n",
    "train_data = dataset_split[\"train\"]\n",
    "val_data = dataset_split[\"test\"]\n",
    "\n",
    "data_collator = transformers.DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_data, batch_size=parameters.batch_size, \n",
    "    shuffle=False, collate_fn=data_collator, num_workers=0\n",
    ")\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_data, batch_size=parameters.batch_size, \n",
    "    shuffle=False, collate_fn=data_collator, num_workers=0\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|user|>\n",
      "When a water tank is $30\\%$ full, it contains 27 gallons less than when it is $20\\%$ empty. How many gallons of water does the tank hold when it is full?</s> \n",
      "<|assistant|>\n",
      "I want to find the total capacity of the tank, so I will call that C.\n",
      "Then, when the tank is $30\\%$ full, it has $0.3C$ gallons of water, and when it is $20\\%$ empty, it has $0.8C$ gallons of water.\n",
      "The problem says that the difference between these two amounts is 27 gallons, so I can write an equation: $0.8C - 0.3C = 27$.\n",
      "Simplifying the equation, I get $0.5C = 27$, so $C = 54$.\n",
      "Therefore, the tank holds 54 gallons of water when it is full.\n",
      "# Answer\n",
      "\n",
      "54</s> \n",
      "</s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s>\n"
     ]
    }
   ],
   "source": [
    "it = iter(train_loader)\n",
    "batch = next(it)\n",
    "print(tokenizer.decode(batch[\"positive_input_ids\"][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_269980/2786336696.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(os.path.join(checkpoint_dir, \"base_model.pt\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138.431232 M parameters\n"
     ]
    }
   ],
   "source": [
    "from files.llm import Llama, ModelArgs\n",
    "\n",
    "checkpoint = torch.load(os.path.join(checkpoint_dir, \"base_model.pt\"))\n",
    "config = checkpoint.pop(\"config\")\n",
    "\n",
    "model_args = ModelArgs(\n",
    "    dim=config.hidden_size,\n",
    "    n_layers=config.num_hidden_layers,\n",
    "    n_heads=config.num_attention_heads,\n",
    "    n_kv_heads=config.num_key_value_heads,\n",
    "    vocab_size=config.vocab_size,\n",
    "    norm_eps=config.rms_norm_eps,\n",
    "    rope_theta=config.rope_theta,\n",
    "    max_seq_len=parameters.context_length,\n",
    "    dropout=config.attention_dropout,\n",
    "    hidden_dim=config.intermediate_size,\n",
    "    attention_bias=config.attention_bias,\n",
    "    mlp_bias=config.mlp_bias,\n",
    ")\n",
    "\n",
    "model = Llama(model_args)\n",
    "model.load_state_dict(checkpoint)\n",
    "model = model.to(parameters.dtype)\n",
    "model = model.to(parameters.device)\n",
    "model.train()\n",
    "\n",
    "if parameters.compile:\n",
    "    print(\"[INFO] Compiling model\")\n",
    "    model = torch.compile(model)\n",
    "\n",
    "print(sum(p.numel() for p in model.parameters()) / 1e6, \"M parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 111408 steps\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(), lr=parameters.learning_rate, betas=(0.9, 0.95), eps=1e-8, \n",
    "    fused=(parameters.device.type == \"cuda\"), weight_decay=parameters.weight_decay,\n",
    ")\n",
    "\n",
    "num_trainings_steps = len(train_loader) * parameters.epochs\n",
    "print(f\"Training for {num_trainings_steps} steps\")\n",
    "\n",
    "def lr_lambda(current_step: int) -> float:\n",
    "    if current_step < parameters.lr_warmup_steps:\n",
    "        return float(current_step) / float(max(1, parameters.lr_warmup_steps))\n",
    "    \n",
    "    progress = float(current_step - parameters.lr_warmup_steps) / float(\n",
    "        max(1, num_trainings_steps - parameters.lr_warmup_steps)\n",
    "    )\n",
    "\n",
    "    return max(0.0, 0.5 * (1.0 + math.cos(math.pi * progress)))\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda, last_epoch=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_logprops(prompt_attention_mask, chosen_inputs, chosen_attention_mask, logits):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
