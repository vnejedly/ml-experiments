{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/frood/sources/jupyter-server/venv/lib/python3.12/site-packages/torch/_subclasses/functional_tensor.py:275: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:81.)\n",
      "  cpu = _conversion_method_template(device=torch.device(\"cpu\"))\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "import ipdb\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import platform, shutil\n",
    "import requests, zipfile, io\n",
    "from typing import Tuple, Iterator\n",
    "\n",
    "# Pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# tokenizer\n",
    "import sentencepiece as spm\n",
    "\n",
    "# improve performance for Ampere GPU architecture\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "# empty GPU cache\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "class Config:\n",
    "    def __init__(self):\n",
    "        # Architecture parameters\n",
    "        self.batch_size = 64\n",
    "        self.context_length = 512\n",
    "        self.embed_size = 384\n",
    "        self.n_layers = 7\n",
    "        self.n_heads = 7\n",
    "        self.bias = True\n",
    "\n",
    "        # Hyperparameters\n",
    "        self.lr = 0.0001\n",
    "        self.dropout = 0.05\n",
    "        self.weight_decay = 0.01\n",
    "        self.grad_clip = 1.0\n",
    "\n",
    "        # Training parameters\n",
    "        self.train_iters = 10000\n",
    "        self.eval_interval = 50\n",
    "        self.compile = False\n",
    "        self.local_data_path = \"data/llm\" # path when running on the cuda server\n",
    "        # self.local_data_path = \"../../data/llm\" # path when running on local machine\n",
    "        self.checkpoint_dir_name = \"models\"  # Replace with your actual path\n",
    "        self.checkpoint_filename = \"llm_latest.pt\"\n",
    "        self.load_pretrained = True\n",
    "        self.dtype = torch.bfloat16\n",
    "\n",
    "        # Mode\n",
    "        self.inference = False\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    @property\n",
    "    def checkpoint_dir(self) -> str:\n",
    "        return os.path.join(self.local_data_path, self.checkpoint_dir_name)\n",
    "    \n",
    "    @property\n",
    "    def checkpoint_file_path(self) -> str:\n",
    "        return os.path.join(self.checkpoint_dir, self.checkpoint_filename)\n",
    "    \n",
    "    def get_file_path(self, file_name: str) -> str:\n",
    "        return os.path.join(self.local_data_path, file_name)\n",
    "\n",
    "\n",
    "config = Config()\n",
    "\n",
    "print(f\"Device: {config.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# files_url = \"https://ideami.com/llm_train\"\n",
    "\n",
    "# print(\"Downloading files from\", files_url)\n",
    "# response = requests.get(files_url)\n",
    "\n",
    "# print(\"Extracting files to\", config.local_data_path)\n",
    "# zip_file = zipfile.ZipFile(io.BytesIO(response.content)).extractall(config.local_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/frood/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mvladislav-nejedly\u001b[0m (\u001b[33mvladislav-nejedly-none\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/frood/sources/jupyter-server/wandb/run-20250202_223407-4lphndax</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/vladislav-nejedly-none/llm9/runs/4lphndax' target=\"_blank\">llm9run</a></strong> to <a href='https://wandb.ai/vladislav-nejedly-none/llm9' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/vladislav-nejedly-none/llm9' target=\"_blank\">https://wandb.ai/vladislav-nejedly-none/llm9</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/vladislav-nejedly-none/llm9/runs/4lphndax' target=\"_blank\">https://wandb.ai/vladislav-nejedly-none/llm9/runs/4lphndax</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# loggigng\n",
    "wandb_log = True\n",
    "wandb_project = \"llm9\"\n",
    "wandb_run_name = \"llm9run\"\n",
    "# wandb_run_name = wandb_project + \"-\" + datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\")\n",
    "\n",
    "if wandb_log:\n",
    "    import wandb\n",
    "    wandb.login(key=\"***\") # paste a valid API key\n",
    "    wandb.init(project=wandb_project, name=wandb_run_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "terms.\n",
      "For example, there are objects in two groups (as shown on the right). The objects are various shapes, where one group has 3 of them while the other has 2. When the two groups combine into one, the overall amount (sum) of the shapes become 5.\n",
      "\n",
      "Vertical Addition\n",
      "\n",
      "The animation above demonstrate\n"
     ]
    }
   ],
   "source": [
    "with open(\n",
    "    config.get_file_path(os.path.join(\"wiki.txt\")), \n",
    "    \"r\", encoding=\"utf-8\"\n",
    ") as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(text[30000:30300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer vocab_size: 4096\n"
     ]
    }
   ],
   "source": [
    "sp = spm.SentencePieceProcessor(\n",
    "    model_file=config.get_file_path(\"wiki_tokenizer.model\")\n",
    ")\n",
    "\n",
    "vocab_size = sp.get_piece_size()\n",
    "print(f\"Tokenizer vocab_size: {vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2686, 698, 265, 261, 684]\n",
      "once upon a time\n"
     ]
    }
   ],
   "source": [
    "encode = lambda s: sp.Encode(s)\n",
    "decode = lambda l: sp.Decode(l)\n",
    "\n",
    "print(encode(\"once upon a time\"))\n",
    "print(decode([2686, 698, 265, 261, 684]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading encodeed data\n"
     ]
    }
   ],
   "source": [
    "encoded_data_path = config.get_file_path(\"encoded_data.pt\")\n",
    "\n",
    "if os.path.exists(encoded_data_path):\n",
    "    print(\"Loading encodeed data\")\n",
    "    data = torch.load(encoded_data_path)\n",
    "else:\n",
    "    print(\"Encoding data\")\n",
    "    data = torch.tensor(encode(text), dtype=torch.long)\n",
    "    torch.save(data, encoded_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total data: 59.21 Million | Training: 53.29 Million | Validation: 5.92 Million\n"
     ]
    }
   ],
   "source": [
    "data_size = len(data)\n",
    "spl = int(0.9 * data_size)\n",
    "train_data = data[:spl]\n",
    "val_data = data[spl:]\n",
    "\n",
    "print(\n",
    "    f'Total data: {data_size/1e6:.2f} Million | '\n",
    "    f'Training: {len(train_data)/1e6:.2f} Million | '\n",
    "    f'Validation: {len(val_data)/1e6:.2f} Million'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 512]) torch.Size([64, 512])\n",
      "tensor([2992,  436,  389,  280,  964,  561,  700, 1061,  278,  264],\n",
      "       device='cuda:0')\n",
      "tensor([ 436,  389,  280,  964,  561,  700, 1061,  278,  264,  308],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# POTENTIAL PROBLEMS:\n",
    "# Batches can overlap? The same batches can repeat due to the nature \n",
    "# of the random generator? It is uncertain, how big training dataset \n",
    "# will be used in reality...?\n",
    "\n",
    "def get_batch(\n",
    "    data: torch.Tensor,\n",
    "    config: Config,\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    inds = torch.randint(len(data) - config.context_length, (config.batch_size,))\n",
    "    x = torch.stack([data[i:i+config.context_length] for i in inds])\n",
    "    y = torch.stack([data[i+1:i+config.context_length+1] for i in inds])\n",
    "    \n",
    "    return x.to(config.device), y.to(config.device)\n",
    "\n",
    "x, y = get_batch(train_data, config)\n",
    "\n",
    "print(x.shape, y.shape)\n",
    "print(x[0, :10])\n",
    "print(y[0, :10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ForwardLayer(nn.Module):\n",
    "    def __init__(self, config: Config):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(config.embed_size, 6 * config.embed_size, bias=config.bias),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(6 * config.embed_size, config.embed_size, bias=config.bias),\n",
    "            nn.Dropout(config.dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    def __init__(\n",
    "        self, head_size: int, config: Config\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.queries = nn.Linear(config.embed_size, head_size, bias=config.bias)\n",
    "        self.keys = nn.Linear(config.embed_size, head_size, bias=config.bias)\n",
    "        self.values = nn.Linear(config.embed_size, head_size, bias=config.bias)\n",
    "\n",
    "        self.register_buffer(\"tril\", torch.tril(\n",
    "              torch.ones(config.context_length, config.context_length)\n",
    "        ))\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        batch_size, sequence_length, embed_size = x.shape\n",
    "\n",
    "        queries = self.queries(x) # BS, SL, 54\n",
    "        keys = self.keys(x) # BS, SL, 54\n",
    "        values = self.values(x) # BS, SL, 54\n",
    "\n",
    "        attn_w = queries @ keys.transpose(-2, -1) * keys.shape[-1] ** -0.5 # BS, SL, SL\n",
    "        attn_w = attn_w.masked_fill(self.tril[:sequence_length, :sequence_length] == 0, float(\"-inf\"))\n",
    "        attn_w = F.softmax(attn_w, dim=-1) # BS, SL, SL\n",
    "\n",
    "        return attn_w @ values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Multihead(nn.Module):\n",
    "    def __init__(self, config: Config):\n",
    "        super().__init__()\n",
    "\n",
    "        head_size = config.embed_size // config.n_heads\n",
    "\n",
    "        self.heads = nn.ModuleList([\n",
    "            Head(head_size, config)\n",
    "            for _ in range(config.n_heads)\n",
    "        ])\n",
    "\n",
    "        self.combine = nn.Linear(\n",
    "            head_size * config.n_heads, \n",
    "            config.embed_size, bias=config.bias\n",
    "        )\n",
    "        \n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = torch.cat([head(x) for head in self.heads], dim=-1)\n",
    "        # each head returns tensor of shape (batch_size, context_length, head_size)\n",
    "        x = self.combine(x) # (batch_size, context_length, n_heads * head_size ->  batch_size, context_length, embed_size)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, config: Config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.ma = Multihead(config)\n",
    "        self.feed_forward = ForwardLayer(config)\n",
    "        self.ln1 = nn.LayerNorm(config.embed_size)\n",
    "        self.ln2 = nn.LayerNorm(config.embed_size)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = x + self.ma(self.ln1(x))\n",
    "        x = x + self.feed_forward(self.ln2(x))   \n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    def __init__(self, vocab_size: int, config: Config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.config = config\n",
    "\n",
    "        self.embeddings = nn.Embedding(vocab_size, config.embed_size)\n",
    "        self.positions = nn.Embedding(config.context_length, config.embed_size)\n",
    "        self.blocks = nn.Sequential(*[Block(config) for _ in range(config.n_layers)])\n",
    "        self.ln = nn.LayerNorm(config.embed_size)\n",
    "        self.final_linear = nn.Linear(config.embed_size, vocab_size, bias=config.bias)\n",
    "\n",
    "        self.to(config.device)\n",
    "        self.to(config.dtype)\n",
    "        \n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    # initialize the weights of the model\n",
    "    def _init_weights(self, module: nn.Module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            nn.init.normal_(module.weight, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                nn.init.zeros_(module.bias)\n",
    "                \n",
    "        if isinstance(module, nn.Embedding):\n",
    "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(\n",
    "        self, \n",
    "        input: torch.Tensor, \n",
    "        targets: torch.Tensor = None\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        # BS = batch size, SL = sequence (context) length\n",
    "        loss = None\n",
    "        batch_size, sequence_length = input.shape # BS x SL\n",
    "\n",
    "        emb = self.embeddings(input) # BS x SL x 384\n",
    "        pos = self.positions(torch.arange(sequence_length, device=self.config.device)) # SL x 384\n",
    "        x = emb + pos # BS x SL x 384 (pos being broadcasted along the BS dimension)\n",
    "        x = self.blocks(x) # BS x SL x 384\n",
    "        x = self.ln(x) # BS x SL x 384\n",
    "        logits = self.final_linear(x) # BS x SL x VS (vocab size = 4096)\n",
    "\n",
    "        if targets is not None:\n",
    "            batch_size, sequence_length, vocab_size = logits.shape # BS x SL x VS\n",
    "\n",
    "            loss = F.cross_entropy(\n",
    "                logits.view(batch_size * sequence_length, vocab_size),\n",
    "                targets.view(batch_size * sequence_length)     \n",
    "                # Second arg can be either grand truth class index or one-hot encoded \n",
    "                # all classes. In this case, it is the grand truth class index and\n",
    "                # therefore it is 1D opposed to the first arg which is 2D.\n",
    "            )\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, input: torch.Tensor, max: int = 512):\n",
    "        for _ in range(max):\n",
    "            input = input[:, -self.config.context_length:] # (1, input length until max context length)\n",
    "            logits, _ = self(input) # (1, input length, vocab size)\n",
    "            logits = logits[:, -1, :] # pick predicction for the last one token (1, vocab size)\n",
    "            probs = F.softmax(logits, dim=-1) # (1, vocab size)\n",
    "            next = torch.multinomial(probs, num_samples=1) # (take one sample from the probability distribution)\n",
    "            input = torch.cat([input, next], dim=-1) # concatenate the next token to the input\n",
    "        \n",
    "        return input\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def calculate_loss(\n",
    "        self, train_data: torch.Tensor, \n",
    "        val_data: torch.Tensor, get_batch: callable\n",
    "    ) -> Tuple[float, float]:\n",
    "        out = []\n",
    "        self.eval()\n",
    "        eval_iters = 100\n",
    "        \n",
    "        for split in [train_data, val_data]:\n",
    "            l = torch.zeros(eval_iters)\n",
    "            for i in range(eval_iters):\n",
    "                x, y = get_batch(split, self.config)\n",
    "                _, loss = self(x, y)\n",
    "                l[i] = loss\n",
    "            \n",
    "            out.append(l.mean().item())\n",
    "        \n",
    "        self.train()\n",
    "        return tuple(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate_sample(model: GPT, input: str) -> str:\n",
    "    t1 = torch.tensor(encode(input), dtype=torch.long, device=config.device)\n",
    "    t1 = t1[None, :] # add batch dimension\n",
    "    newgen = model.generate(t1, max=64).tolist()\n",
    "    return decode(newgen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19.837954 Million parameters\n",
      "(8.4243745803833, 8.426875114440918)\n"
     ]
    }
   ],
   "source": [
    "model = GPT(vocab_size, config)\n",
    "\n",
    "# If torch.compile(model) is called, the state_dict deom previous model version\n",
    "# which was not compiled is not compatible with the compiled model. After compiling,\n",
    "# the prefix _orig_mod. will be required before each key in the state_dict.\n",
    "if config.compile:\n",
    "    print(\"Torch :: Compiling model\")\n",
    "    model = torch.compile(model)\n",
    "\n",
    "print(sum(p.numel() for p in model.parameters()) / 1e6, \"Million parameters\")\n",
    "print(model.calculate_loss(train_data, val_data, get_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the optimizer\n",
    "p_dict = {p_name: p for p_name, p in model.named_parameters() if p.requires_grad}\n",
    "\n",
    "weight_decay_p = [p for n, p in p_dict.items() if p.dim() >= 2]\n",
    "no_weight_decay_p = [p for n, p in p_dict.items() if p.dim() < 2]\n",
    "\n",
    "optimizer_groups = [\n",
    "    {\"params\": weight_decay_p, \"weight_decay\": config.weight_decay},\n",
    "    {\"params\": no_weight_decay_p, \"weight_decay\": 0.0}\n",
    "]\n",
    "\n",
    "optimizer = torch.optim.AdamW(optimizer_groups, lr=config.lr, betas=(0.9, 0.99))\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer, config.train_iters, eta_min=config.lr / 10\n",
    ")\n",
    "\n",
    "start_iteration = 0\n",
    "best_val_loss = float(\"inf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM - Loading model\n",
      "Loaded iteration 9999 with loss 4.061093807220459\n"
     ]
    }
   ],
   "source": [
    "def load_checkpoint(path: str):\n",
    "    print(\"LLM - Loading model\")\n",
    "    checkpoint = torch.load(path)\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "    iteration = checkpoint[\"iteration\"]\n",
    "    loss = checkpoint[\"loss\"]\n",
    "    print(f\"Loaded iteration {iteration} with loss {loss}\")\n",
    "    return iteration, loss\n",
    "\n",
    "if os.path.exists(config.checkpoint_file_path) and config.load_pretrained:\n",
    "    start_iteration, loss = load_checkpoint(config.checkpoint_file_path)\n",
    "    best_val_loss = loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ttraining loop\n",
    "# try:\n",
    "#     for iteration in tqdm(range(start_iteration, config.train_iters)):\n",
    "#         xb, yb = get_batch(train_data, config)\n",
    "#         logits, loss = model(xb, yb)\n",
    "\n",
    "#         if (iteration % config.eval_interval == 0 or iteration == config.train_iters - 1):\n",
    "#             train_loss, val_loss = model.calculate_loss(train_data, val_data, get_batch)\n",
    "            \n",
    "#             print(\n",
    "#                 f\"Iteration {iteration} | \"\n",
    "#                 f\"Train loss: {train_loss:.3f} | \"\n",
    "#                 f\"Val loss: {val_loss:.3f}\"\n",
    "#             )\n",
    "\n",
    "#             sample = generate_sample(model, \"Once upon a time\")\n",
    "\n",
    "#             torch.save({\n",
    "#                 \"model_state_dict\": model.state_dict(),\n",
    "#                 \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "#                 \"loss\": val_loss,\n",
    "#                 \"iteration\": iteration,\n",
    "#             }, config.checkpoint_file_path)\n",
    "\n",
    "#         if wandb_log:\n",
    "#             wandb.log({\n",
    "#                 \"train_loss\": train_loss,\n",
    "#                 \"val_loss\": val_loss,\n",
    "#                 \"lr\": scheduler.get_last_lr()[0],\n",
    "#             }, step=iteration)\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "\n",
    "#         nn.utils.clip_grad_norm_(model.parameters(), max_norm=config.grad_clip)\n",
    "\n",
    "#         optimizer.step()\n",
    "#         scheduler.step()\n",
    "\n",
    "# except KeyboardInterrupt:\n",
    "#     print(\"Training interrupted. Cleaning up...\")\n",
    "\n",
    "# finally:\n",
    "#     torch.cuda.empty_cache()\n",
    "#     print(\"GPU memory released\")\n",
    "\n",
    "#     if wandb_log:\n",
    "#         wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Once upon a time of agoral imwell better obround, Tobleedific eq functable crocked in the top of the better rement that as born in 1949, and Jennce were killed by Capiio and Clhaanti� Englishings.\\n\\nThe']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;34mwandb\u001b[0m: \n",
      "\u001b[1;34mwandb\u001b[0m: 🚀 View run \u001b[33mllm9run\u001b[0m at: \u001b[34mhttps://wandb.ai/vladislav-nejedly-none/llm9/runs/4lphndax\u001b[0m\n",
      "\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20250202_223407-4lphndax/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "sample = generate_sample(model, \"Once upon a time\")\n",
    "\n",
    "print(sample)\n",
    "\n",
    "# x, y = get_batch(train_data, batch_size, context_length, device)\n",
    "# print(x.shape, y.shape)\n",
    "# print(x[0, :10])\n",
    "# print(y[0, :10])\n",
    "\n",
    "# model = GPT(\n",
    "#     vocab_size=vocab_size, \n",
    "#     embed_size=embed_size, \n",
    "#     context_length=context_length, \n",
    "#     n_layers=n_layers, \n",
    "#     n_heads=n_heads, \n",
    "#     device=device,\n",
    "#     bias=BIAS\n",
    "# )\n",
    "\n",
    "# model.to(dtype)\n",
    "# model.to(device)\n",
    "\n",
    "# x = x.to(device)\n",
    "\n",
    "# logits, loss = model(x, y)\n",
    "\n",
    "# print(logits.shape, loss)\n",
    "\n",
    "# sample = generate_sample(model, \"Once upon a time\")\n",
    "\n",
    "# print(sample)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
